# Case-study оптимизации

## Актуальная проблема
Дана программа на `ruby`, которая обработывает файл с данными. 

Программа успешно работает с небольшими файлами в 657 байт (18 строк), но при попытке передать на обработку файл 128 
мегабайт (3250940 строк) не удаётся дождаться выполнения программы. При этом нет гарантий, что обработка файла когда-нибудь будет 
завершена.

Я решил исправить эту проблему, оптимизировав эту программу.

### Мантры оптимизации:
1) **"Оптимизировать только когда метрика не укладывается в бюджет"**

В данном случае действительно метрика не укладывается в бюджет.
Метрикой в данном задании выступает время обработки файла.
Требуется обрабатывать файл размером 128 мегабайт хотябы меньше, чем за 30 секунд.
Будем ориентироваться на этот бюджет.
В настоящий момент время обратоки составляет более 15 минут, и нет уверенности, что обратока будет когда-нибудь 
выполнена.

Итог:

Для обрабатываемого файла в 128 мегабайт  следующие значения:
* Метрика - время обработки файла в 128 мегабайт
* Бюджет - максимум 30 секунд на обработку файла в 128 мегабайт

2) **"Оптимизировать только главную точку роста"**

Нельзя оптимизировать в слепую всё подряд, нужно точно определить проблемные места.
В противном случае есть риск демотивации.

Итог:
* Нужно найти главную точку роста.

### Общий фреймврок оптимизации
- Защищаем поведение системы от внесения ошибок тестом (есть вероятность сломать какой-то участок кода при оптимизации, поэтому стоит защищать оптимизуемые участки кода);
- Сортируем проблемы по степени значимости (выбираем главную проблему и концентрируемся на ней);
- Фиксируем/формируем метрику
- Фиксируем/формируем бюджет на метрику
- Защищаем метрику от дальнейшей деградации тестом (пишем тест на производительность);
- Профилируем, вникаем в детали системы чтобы найти главную точку роста
- Выстраивам Feedback-Loop (мы нашли главную точку роста и пытаемся улучшить метрику системы)
    - Loop: профилируем -> вносим изменения -> запускаем тесты -> измеряем (benchmark) -> commit/revert
- Обновляем тест для защиты достигнутого прогресса
- Формируем case-study, считаем $$

### Анализ асимптотики
Беглый анализ асимптотики. 
Программа характеризуется объёмом входных даннных. 
Сейчас практически невозможно узнать время выполнения обработки файла `data_large.txt` поэтому для начала стоит взять 
четыре файла с отличяющимся колличеством строк (я возьму 10000, 20000, 30000 и 40000) и посмотреть отличия времени обработки для этих 
файлов.
Таким образом можно примерно увидеть алгоритмическую сложность.

## Формирование метрики
Для того, чтобы понимать, оказывают ли мои изменения положительный эффект на быстродействие программы я придумал 
использовать такую метрику: **время обработки файла в 128 мегабайт**

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. 
Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации. 

Однако я возьму на себя смелость перенести тест на фреймворк RSpec для удобства. Так как далее согласно фреймворку 
оптимизации потребуется защитить метрику от деградации, удобнее это сделать с помощью `rspec-benchmark`. 
Также на первом этапе я разделил исходный файл `task-1.rb` на `lib/handling/user.rb` (содержит класс User) и 
`lib/handling/user_repository.rb` (содержит функциональность обработки файла и записи отчёта в json) сами функции 
**изменению не подверглись**. 
Тестирование обработки файла перенесено в `spec/lib/handling/user_repository_spec.rb`.


## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 5 минут

Вот как я построил `feedback_loop`: *как вы построили feedback_loop*
1) Проведём анализ асимптотики. 
Так как "программа характеризется объёмом входых данных" создам 4-е файла объёмом 10_000 строк (1x), 20_000 (2x), 30_000 (3x) и 40_000 строк (4x) и запущу banchmark для выявления асимптотики.
За исходный файл возьму data_large.txt и воспользуюсь следующей командой:
```shell
head -n N data_large.txt > dataN.txt
```

2) 

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *инструментами, которыми вы воспользовались*
- rbspy, он показал что главной точкой роста является block в функции work, он захватывает более 95% всего времени CPU 
(однако функция имеет несколько блоков) её стоит декомпозировать на несколько функций.

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- какой отчёт показал главную точку роста
- как вы решили её оптимизировать
- как изменилась метрика
- как изменился отчёт профилировщика - исправленная проблема перестала быть главной точкой роста?